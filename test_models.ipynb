{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2951421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выход модели: torch.Size([4, 32, 1])\n"
     ]
    }
   ],
   "source": [
    "# src/ffn/ffn.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Позиционные эмбеддинги для захвата временной информации.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Механизм внимания для фокусировки на важных временных шагах.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, num_heads=8, dropout=0.1):\n",
    "        super(Attention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, hidden_size]\n",
    "        Returns:\n",
    "            Tensor, shape [batch_size, seq_len, hidden_size]\n",
    "        \"\"\"\n",
    "        attn_output, _ = self.multihead_attn(x, x, x)\n",
    "        # Остаточное соединение и нормализация\n",
    "        x = self.norm(x + self.dropout(attn_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TradingFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Мощная Feed-Forward Network для прогнозирования цен на фондовом рынке.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_size=256,\n",
    "        hidden_sizes=[512, 1024, 512, 256],\n",
    "        output_size=32,\n",
    "        seq_len=256,\n",
    "        num_attention_layers=3,\n",
    "        num_heads=8,\n",
    "        dropout=0.2,\n",
    "        use_layer_norm=True\n",
    "    ):\n",
    "        super(TradingFFN, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.seq_len = seq_len\n",
    "        self.output_size = output_size\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "\n",
    "        # Позиционное кодирование\n",
    "        self.pos_encoding = PositionalEncoding(feature_size, max_len=seq_len, dropout=dropout)\n",
    "        \n",
    "        # Стек слоев внимания\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            Attention(feature_size, num_heads, dropout)\n",
    "            for _ in range(num_attention_layers)\n",
    "        ])\n",
    "        \n",
    "        # LayerNorm после внимания (опционально)\n",
    "        if use_layer_norm:\n",
    "            self.attn_layer_norm = nn.LayerNorm(feature_size)\n",
    "        \n",
    "        # Стек полносвязных слоев\n",
    "        ff_layers = []\n",
    "        input_dim = feature_size * seq_len\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            ff_layers.extend([\n",
    "                nn.Linear(input_dim, hidden_size),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(hidden_size) if use_layer_norm else nn.Identity(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            input_dim = hidden_size\n",
    "            \n",
    "        self.feed_forward = nn.Sequential(*ff_layers)\n",
    "        \n",
    "        # Выходной слой\n",
    "        self.output_layer = nn.Linear(input_dim, output_size)\n",
    "        \n",
    "        # Инициализация весов\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Инициализация весов.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, src, tgt=None):\n",
    "        \"\"\"\n",
    "        Прямой проход модели.\n",
    "        \n",
    "        Args:\n",
    "            src (torch.Tensor): Исторические данные [B, T_hist, F_hist=256].\n",
    "            tgt (torch.Tensor, optional): Целевые значения [B, T_pred, 1].\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Прогнозы [B, T_pred, 1].\n",
    "        \"\"\"\n",
    "        batch_size = src.size(0)\n",
    "        \n",
    "        # Применяем позиционное кодирование\n",
    "        x = self.pos_encoding(src)  # [B, 256, 256]\n",
    "        \n",
    "        # Пропускаем через слои внимания\n",
    "        for attn_layer in self.attention_layers:\n",
    "            x = attn_layer(x)  # [B, 256, 256]\n",
    "            \n",
    "        # Применяем LayerNorm после внимания (опционально)\n",
    "        if self.use_layer_norm:\n",
    "            x = self.attn_layer_norm(x)\n",
    "        \n",
    "        # Сглаживаем для полносвязных слоев\n",
    "        x = x.view(batch_size, -1)  # [B, 256*256]\n",
    "        \n",
    "        # Пропускаем через полносвязные слои\n",
    "        features = self.feed_forward(x)  # [B, hidden_sizes[-1]]\n",
    "        \n",
    "        # Выходной слой\n",
    "        output = self.output_layer(features)  # [B, 32]\n",
    "        \n",
    "        return output.unsqueeze(-1)  # [B, 32, 1]\n",
    "\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    # Параметры модели\n",
    "    B, T_hist, feature_size = 4, 256, 256\n",
    "    T_pred = 32\n",
    "    output_size = 1\n",
    "    \n",
    "    # Создание модели\n",
    "    model = TradingFFN(\n",
    "        feature_size=feature_size,\n",
    "        hidden_sizes=[512, 1024, 512, 256],\n",
    "        output_size=T_pred,\n",
    "        seq_len=T_hist,\n",
    "        num_attention_layers=3,\n",
    "        num_heads=8,\n",
    "        dropout=0.2,\n",
    "        use_layer_norm=True\n",
    "    )\n",
    "    \n",
    "    # Примерные входные данные\n",
    "    src = torch.randn(B, T_hist, feature_size)  # История после TradingProcessor\n",
    "    tgt = torch.randn(B, T_pred, output_size)   # Целевые значения (опционально)\n",
    "    \n",
    "    # Прогон модели\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(src, tgt)\n",
    "        print(f\"Выход модели: {output.shape}\")  # [B, 32, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5886f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
